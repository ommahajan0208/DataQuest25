{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee52baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Memory reduced: 3.74MB ‚Üí 1.35MB (63.8%)\n",
      "‚úÖ Data ready: (14522, 19) (3631, 19)\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# --- Memory Optimization ---\n",
    "def reduce_memory_usage(df):\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type == object:\n",
    "            if df[col].nunique() / len(df[col]) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "        elif col_type.name.startswith('int'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        elif col_type.name.startswith('float'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"üîß Memory reduced: {start_mem:.2f}MB ‚Üí {end_mem:.2f}MB ({100*(start_mem-end_mem)/start_mem:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "# --- Load & preprocess ---\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = reduce_memory_usage(df)\n",
    "\n",
    "X = df.drop(columns=['ASI_category', 'ID'])\n",
    "y = df['ASI_category'].astype('category').cat.codes\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data ready:\", X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3592a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Training base models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:53:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3847\n",
      "[LightGBM] [Info] Number of data points in the train set: 14522, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.755382\n",
      "[LightGBM] [Info] Start training from score -0.355142\n",
      "[LightGBM] [Info] Start training from score -2.070802\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid_0's multi_logloss: 0.157847\n",
      "‚úÖ Base models trained.\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# --- Base models ---\n",
    "best_xgb = XGBClassifier(\n",
    "    n_estimators=800, learning_rate=0.045, max_depth=8,\n",
    "    subsample=0.7, colsample_bytree=0.8, random_state=42,\n",
    "    eval_metric=\"mlogloss\", n_jobs=-1, use_label_encoder=False\n",
    ")\n",
    "best_lgbm = LGBMClassifier(\n",
    "    n_estimators=400, learning_rate=0.055, num_leaves=40,\n",
    "    max_depth=14, subsample=0.85, colsample_bytree=0.95,\n",
    "    reg_alpha=1e-5, reg_lambda=0.02, min_child_samples=18,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=600, max_depth=10, min_samples_split=4,\n",
    "    min_samples_leaf=2, max_features=0.8, criterion=\"log_loss\",\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"üîß Training base models...\")\n",
    "best_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "best_lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "              callbacks=[early_stopping(80), log_evaluation(0)])\n",
    "best_rf.fit(X_train, y_train)\n",
    "print(\"‚úÖ Base models trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c1c6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Best weights found: (np.float64(1.0), np.float64(0.2), np.float64(0.1)) | F1 = 0.9227\n",
      "\n",
      "‚úÖ Ensemble Performance:\n",
      "Accuracy: 0.9435 | F1: 0.9227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       628\n",
      "           1       0.96      0.97      0.96      2546\n",
      "           2       0.92      0.89      0.91       457\n",
      "\n",
      "    accuracy                           0.94      3631\n",
      "   macro avg       0.93      0.92      0.92      3631\n",
      "weighted avg       0.94      0.94      0.94      3631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# --- Weighted ensemble optimization ---\n",
    "from itertools import product\n",
    "\n",
    "xgb_p, lgbm_p, rf_p = best_xgb.predict_proba(X_val), best_lgbm.predict_proba(X_val), best_rf.predict_proba(X_val)\n",
    "grid = np.arange(0.1, 1.1, 0.1)\n",
    "best_f1, best_w = 0, None\n",
    "\n",
    "for w1, w2, w3 in product(grid, repeat=3):\n",
    "    probs = (w1*xgb_p + w2*lgbm_p + w3*rf_p) / (w1+w2+w3)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    f1 = f1_score(y_val, preds, average=\"macro\")\n",
    "    if f1 > best_f1: best_f1, best_w = f1, (w1, w2, w3)\n",
    "\n",
    "print(f\"üî• Best weights found: {best_w} | F1 = {best_f1:.4f}\")\n",
    "final_probs = (best_w[0]*xgb_p + best_w[1]*lgbm_p + best_w[2]*rf_p) / sum(best_w)\n",
    "val_preds = np.argmax(final_probs, axis=1)\n",
    "print(\"\\n‚úÖ Ensemble Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, val_preds):.4f} | F1: {f1_score(y_val, val_preds, average='macro'):.4f}\")\n",
    "print(classification_report(y_val, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dde228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Calibrating base models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Calibrated Meta Accuracy: 0.9433\n",
      "üèÅ Calibrated Meta F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# --- Calibration + Meta-stacking ---\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"‚öñÔ∏è Calibrating base models...\")\n",
    "cal_xgb = CalibratedClassifierCV(best_xgb, method=\"isotonic\", cv=\"prefit\")\n",
    "cal_lgbm = CalibratedClassifierCV(best_lgbm, method=\"isotonic\", cv=\"prefit\")\n",
    "cal_rf   = CalibratedClassifierCV(best_rf, method=\"isotonic\", cv=\"prefit\")\n",
    "for model in [cal_xgb, cal_lgbm, cal_rf]:\n",
    "    model.fit(X_val, y_val)\n",
    "\n",
    "stack_X = np.hstack([cal_xgb.predict_proba(X_val), cal_lgbm.predict_proba(X_val), cal_rf.predict_proba(X_val)])\n",
    "meta_lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "meta_lr.fit(stack_X, y_val)\n",
    "\n",
    "stack_preds = np.argmax(meta_lr.predict_proba(stack_X), axis=1)\n",
    "print(f\"üèÅ Calibrated Meta Accuracy: {accuracy_score(y_val, stack_preds):.4f}\")\n",
    "print(f\"üèÅ Calibrated Meta F1: {f1_score(y_val, stack_preds, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780e45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Advanced Ensemble Refinement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Meta-LogReg F1: 0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Pseudo-Labeled F1: 0.9419\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# --- Meta Feature Engineering + Pseudo-Labeling ---\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "print(\"üß™ Running Advanced Ensemble Refinement...\")\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly, X_val_poly = poly.fit_transform(X_train), poly.transform(X_val)\n",
    "scaler = StandardScaler()\n",
    "X_train_poly, X_val_poly = scaler.fit_transform(X_train_poly), scaler.transform(X_val_poly)\n",
    "\n",
    "xgb_p, lgbm_p, rf_p = cal_xgb.predict_proba(X_val), cal_lgbm.predict_proba(X_val), cal_rf.predict_proba(X_val)\n",
    "meta_features = np.hstack([xgb_p, lgbm_p, rf_p, X_val_poly])\n",
    "\n",
    "meta_lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=2.0, max_iter=2000, random_state=42)\n",
    "meta_lr.fit(meta_features, y_val)\n",
    "meta_preds = meta_lr.predict(meta_features)\n",
    "\n",
    "print(f\"üèÅ Meta-LogReg F1: {f1_score(y_val, meta_preds, average='macro'):.4f}\")\n",
    "confidence = np.max(meta_lr.predict_proba(meta_features), axis=1)\n",
    "pseudo_idx = np.where(confidence >= 0.95)[0]\n",
    "\n",
    "if len(pseudo_idx) > 0:\n",
    "    X_val_sel = X_val.iloc[pseudo_idx]\n",
    "    val_aug = np.hstack([\n",
    "        cal_xgb.predict_proba(X_val_sel),\n",
    "        cal_lgbm.predict_proba(X_val_sel),\n",
    "        cal_rf.predict_proba(X_val_sel),\n",
    "        X_val_poly[pseudo_idx]\n",
    "    ])\n",
    "    X_aug = np.vstack([meta_features, val_aug])\n",
    "    y_aug = np.concatenate([y_val, y_val.iloc[pseudo_idx]])\n",
    "    meta_lr.fit(X_aug, y_aug)\n",
    "    final_preds = meta_lr.predict(meta_features)\n",
    "    print(f\"üöÄ Pseudo-Labeled F1: {f1_score(y_val, final_preds, average='macro'):.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pseudo-labels added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990bb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
