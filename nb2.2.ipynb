{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3119ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (18153, 21)\n",
      "Test shape: (7780, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ASI_category</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>Soil_Temperature</th>\n",
       "      <th>Radiation</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Wind_Gusts</th>\n",
       "      <th>...</th>\n",
       "      <th>Surface_Pressure</th>\n",
       "      <th>Relative_Humidity</th>\n",
       "      <th>Soil_Moisture</th>\n",
       "      <th>Dew_Point</th>\n",
       "      <th>Sunshine_Duration</th>\n",
       "      <th>Cloud_Cover</th>\n",
       "      <th>Precipitation_Hours</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Weather_Code</th>\n",
       "      <th>Daylight_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19554</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.931231</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.757673</td>\n",
       "      <td>0.879671</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538056</td>\n",
       "      <td>55</td>\n",
       "      <td>0.546243</td>\n",
       "      <td>17.564597</td>\n",
       "      <td>53252.08</td>\n",
       "      <td>12.136192</td>\n",
       "      <td>1</td>\n",
       "      <td>176.459082</td>\n",
       "      <td>51</td>\n",
       "      <td>58772.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25205</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.566323</td>\n",
       "      <td>0.096715</td>\n",
       "      <td>0.096715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291448</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.532172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568475</td>\n",
       "      <td>88</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>5.692134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>91.901341</td>\n",
       "      <td>16</td>\n",
       "      <td>232.433005</td>\n",
       "      <td>61</td>\n",
       "      <td>28143.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771</td>\n",
       "      <td>Poor</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277340</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>0.189008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706520</td>\n",
       "      <td>78</td>\n",
       "      <td>0.791908</td>\n",
       "      <td>-25.264420</td>\n",
       "      <td>30213.79</td>\n",
       "      <td>18.859670</td>\n",
       "      <td>0</td>\n",
       "      <td>44.688600</td>\n",
       "      <td>3</td>\n",
       "      <td>34621.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635669</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.134048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>57</td>\n",
       "      <td>0.473988</td>\n",
       "      <td>5.913865</td>\n",
       "      <td>44627.21</td>\n",
       "      <td>38.759757</td>\n",
       "      <td>0</td>\n",
       "      <td>333.640418</td>\n",
       "      <td>3</td>\n",
       "      <td>59192.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14036</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.827170</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743855</td>\n",
       "      <td>0.781282</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.391421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546378</td>\n",
       "      <td>50</td>\n",
       "      <td>0.459538</td>\n",
       "      <td>9.661455</td>\n",
       "      <td>45267.17</td>\n",
       "      <td>60.058955</td>\n",
       "      <td>1</td>\n",
       "      <td>86.996954</td>\n",
       "      <td>51</td>\n",
       "      <td>59956.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID ASI_category  Temperature  Precipitation  Rainfall  Snowfall  \\\n",
       "0  19554     Moderate     0.931231       0.000912  0.000912       0.0   \n",
       "1  25205     Moderate     0.566323       0.096715  0.096715       0.0   \n",
       "2    771         Poor     0.018033       0.000000  0.000000       0.0   \n",
       "3   1976         Good     0.717541       0.000000  0.000000       0.0   \n",
       "4  14036     Moderate     0.827170       0.001825  0.001825       0.0   \n",
       "\n",
       "   Soil_Temperature  Radiation  Wind_Speed  Wind_Gusts  ...  Surface_Pressure  \\\n",
       "0          0.757673   0.879671    0.179293    0.193029  ...          0.538056   \n",
       "1          0.291448   0.008913    0.588384    0.532172  ...          0.568475   \n",
       "2          0.000000   0.277340    0.247475    0.189008  ...          0.706520   \n",
       "3          0.635669   0.796709    0.123737    0.134048  ...          0.547500   \n",
       "4          0.743855   0.781282    0.343434    0.391421  ...          0.546378   \n",
       "\n",
       "   Relative_Humidity  Soil_Moisture  Dew_Point  Sunshine_Duration  \\\n",
       "0                 55       0.546243  17.564597           53252.08   \n",
       "1                 88       0.557803   5.692134               0.00   \n",
       "2                 78       0.791908 -25.264420           30213.79   \n",
       "3                 57       0.473988   5.913865           44627.21   \n",
       "4                 50       0.459538   9.661455           45267.17   \n",
       "\n",
       "   Cloud_Cover  Precipitation_Hours  Wind_Direction  Weather_Code  \\\n",
       "0    12.136192                    1      176.459082            51   \n",
       "1    91.901341                   16      232.433005            61   \n",
       "2    18.859670                    0       44.688600             3   \n",
       "3    38.759757                    0      333.640418             3   \n",
       "4    60.058955                    1       86.996954            51   \n",
       "\n",
       "   Daylight_Duration  \n",
       "0           58772.52  \n",
       "1           28143.12  \n",
       "2           34621.43  \n",
       "3           59192.17  \n",
       "4           59956.03  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load train and test data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d6ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train['ASI_category_encoded'] = le.fit_transform(df_train['ASI_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a30a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Combined feature engineering function:\n",
    "    - Includes both base interaction features and advanced meteorological features.\n",
    "    - Ensures safe numerical operations and consistent encodings.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.replace(' ', '_').str.replace('.', '', regex=False)\n",
    "    epsilon = 1e-6  # To avoid division by zero\n",
    "\n",
    "    # --- BASE FEATURES (Your original ones) ---\n",
    "    df['Temp_Range_Impact'] = df['Temperature'] * df['Soil_Temperature']\n",
    "    df['Humidity_Moisture'] = df['Relative_Humidity'] * df['Soil_Moisture']\n",
    "    df['Effective_Radiation'] = df['Radiation'] * (df['Sunshine_Duration'] / (df['Daylight_Duration'] + epsilon))\n",
    "    df['Total_Precip'] = df['Rainfall'] + df['Snowfall'] + df['Precipitation']\n",
    "    df['Wind_Intensity'] = df['Wind_Speed'] * df['Wind_Gusts']\n",
    "    df['Pressure_Humidity_Interaction'] = df['Surface_Pressure'] * df['Relative_Humidity']\n",
    "\n",
    "    # --- EXTENDED INTERACTIONS ---\n",
    "    df['Temp_Diff_Air_Soil'] = df['Temperature'] - df['Soil_Temperature']\n",
    "    df['Temp_Mean'] = (df['Temperature'] + df['Soil_Temperature']) / 2\n",
    "    df['Temp_Humidity_Index'] = df['Temperature'] * df['Relative_Humidity']\n",
    "    df['Radiation_Per_Hour'] = df['Radiation'] / (df['Sunshine_Duration'] + epsilon)\n",
    "    df['Sunshine_Ratio'] = df['Sunshine_Duration'] / (df['Daylight_Duration'] + epsilon)\n",
    "    df['Wind_Stress'] = df['Wind_Speed'] ** 2\n",
    "    df['Wind_Ratio'] = df['Wind_Gusts'] / (df['Wind_Speed'] + epsilon)\n",
    "    df['Humidity_to_Pressure'] = df['Relative_Humidity'] / (df['Surface_Pressure'] + epsilon)\n",
    "    df['Radiation_to_Temp'] = df['Radiation'] / (df['Temperature'] + epsilon)\n",
    "\n",
    "    # --- ADVANCED METEOROLOGICAL FEATURES ---\n",
    "    df['Temperature_Delta'] = df['Temperature'] - df['Dew_Point']\n",
    "    df['Soil_Temp_Air_Temp_Ratio'] = np.where(\n",
    "        df['Temperature'].abs() > epsilon,\n",
    "        df['Soil_Temperature'] / df['Temperature'],\n",
    "        0\n",
    "    )\n",
    "    df['Rainfall_Intensity'] = df['Rainfall'] / (df['Precipitation_Hours'] + epsilon)\n",
    "    df['Pressure_MSL_Difference'] = df['Pressure_MSL'] - df['Surface_Pressure']\n",
    "    df['Wind_Gust_Ratio'] = np.where(\n",
    "        df['Wind_Speed'].abs() > epsilon,\n",
    "        df['Wind_Gusts'] / df['Wind_Speed'],\n",
    "        1.0\n",
    "    )\n",
    "    df['Radiation_Efficiency'] = np.where(\n",
    "        df['Radiation'].abs() > epsilon,\n",
    "        df['Sunshine_Duration'] / df['Radiation'],\n",
    "        0\n",
    "    )\n",
    "    df['Precipitation_Type_Snow'] = np.where(df['Snowfall'] > 0, 1, 0)\n",
    "    df['Precipitation_Ratio_Rain'] = np.where(\n",
    "        df['Precipitation'].abs() > epsilon,\n",
    "        df['Rainfall'] / df['Precipitation'],\n",
    "        0\n",
    "    )\n",
    "    df['Soil_Moisture_RH_Interaction'] = df['Soil_Moisture'] * df['Relative_Humidity']\n",
    "    df['Cloud_Rain_Interaction'] = df['Cloud_Cover'] * df['Rainfall']\n",
    "\n",
    "    # --- Stability and Convection ---\n",
    "    df['Temperature_Inversion_Delta'] = np.abs(df['Soil_Temperature'] - df['Temperature'])\n",
    "    df['Humidity_Saturated_Index'] = df['Relative_Humidity'] * np.where(\n",
    "        df['Temperature'].abs() > epsilon,\n",
    "        df['Dew_Point'] / df['Temperature'],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # --- Wind Shear / Turbulence ---\n",
    "    df['Wind_Force_Index'] = df['Wind_Speed'] ** 2\n",
    "    df['Gust_Delta_Ratio'] = np.where(\n",
    "        df['Wind_Speed'].abs() > epsilon,\n",
    "        (df['Wind_Gusts'] - df['Wind_Speed']) / df['Wind_Speed'],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # --- Daylight & Cloud ---\n",
    "    df['Daylight_Efficiency'] = np.where(\n",
    "        df['Daylight_Duration'].abs() > epsilon,\n",
    "        df['Sunshine_Duration'] / df['Daylight_Duration'],\n",
    "        0\n",
    "    )\n",
    "    df['Cloud_Cover_Inverse'] = 100 - df['Cloud_Cover']\n",
    "\n",
    "    # --- Cyclic Wind Direction ---\n",
    "    if 'Wind_Direction' in df.columns:\n",
    "        wind_rad = np.deg2rad(df['Wind_Direction'])\n",
    "        df['Wind_Direction_Sin'] = np.sin(wind_rad)\n",
    "        df['Wind_Direction_Cos'] = np.cos(wind_rad)\n",
    "\n",
    "    # --- Pressure Trend by ID (conceptual time component) ---\n",
    "    if 'ID' in df.columns:\n",
    "        df = df.sort_values(by=['ID'])\n",
    "        df['Pressure_MSL_Trend'] = df.groupby('ID')['Pressure_MSL'].diff().fillna(0)\n",
    "\n",
    "    # --- Clean NaNs only for numeric columns ---\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    # --- Drop duplicate columns ---\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = feature_engineering(df_train)\n",
    "df_test = feature_engineering(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = df_train.select_dtypes(include=[np.number]).columns.drop('ASI_category_encoded')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "df_test[num_cols] = scaler.transform(df_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43a8931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Important Features:\n",
      "                          Feature  Importance\n",
      "27                      Temp_Mean    0.222537\n",
      "20              Temp_Range_Impact    0.101560\n",
      "22            Effective_Radiation    0.093026\n",
      "1                     Temperature    0.054316\n",
      "34              Radiation_to_Temp    0.047103\n",
      "6                       Radiation    0.042572\n",
      "14              Sunshine_Duration    0.032083\n",
      "5                Soil_Temperature    0.020690\n",
      "13                      Dew_Point    0.015868\n",
      "46       Humidity_Saturated_Index    0.015154\n",
      "35              Temperature_Delta    0.015104\n",
      "19              Daylight_Duration    0.014875\n",
      "26             Temp_Diff_Air_Soil    0.014764\n",
      "30                 Sunshine_Ratio    0.013029\n",
      "49            Daylight_Efficiency    0.012452\n",
      "41        Precipitation_Type_Snow    0.011208\n",
      "45    Temperature_Inversion_Delta    0.010265\n",
      "36       Soil_Temp_Air_Temp_Ratio    0.009613\n",
      "2                   Precipitation    0.009514\n",
      "43   Soil_Moisture_RH_Interaction    0.009478\n",
      "12                  Soil_Moisture    0.008942\n",
      "40           Radiation_Efficiency    0.008875\n",
      "44         Cloud_Rain_Interaction    0.008701\n",
      "21              Humidity_Moisture    0.008562\n",
      "3                        Rainfall    0.008551\n",
      "11              Relative_Humidity    0.008357\n",
      "29             Radiation_Per_Hour    0.008306\n",
      "4                        Snowfall    0.008249\n",
      "28            Temp_Humidity_Index    0.007879\n",
      "37             Rainfall_Intensity    0.007804\n",
      "42       Precipitation_Ratio_Rain    0.007415\n",
      "16            Precipitation_Hours    0.007409\n",
      "50            Cloud_Cover_Inverse    0.007280\n",
      "25  Pressure_Humidity_Interaction    0.007276\n",
      "9                    Pressure_MSL    0.007096\n",
      "7                      Wind_Speed    0.007045\n",
      "33           Humidity_to_Pressure    0.007043\n",
      "15                    Cloud_Cover    0.006835\n",
      "18                   Weather_Code    0.006801\n",
      "0                              ID    0.006704\n",
      "39                Wind_Gust_Ratio    0.006591\n",
      "10               Surface_Pressure    0.006571\n",
      "8                      Wind_Gusts    0.006484\n",
      "32                     Wind_Ratio    0.006403\n",
      "52             Wind_Direction_Cos    0.006400\n",
      "51             Wind_Direction_Sin    0.006387\n",
      "47               Wind_Force_Index    0.006191\n",
      "24                 Wind_Intensity    0.006092\n",
      "17                 Wind_Direction    0.006001\n",
      "23                   Total_Precip    0.005812\n",
      "38        Pressure_MSL_Difference    0.005753\n",
      "31                    Wind_Stress    0.005657\n",
      "48               Gust_Delta_Ratio    0.005320\n",
      "53             Pressure_MSL_Trend    0.000000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Define features and target ---\n",
    "X = df_train.drop(['ASI_category', 'ASI_category_encoded'], axis=1)\n",
    "y = df_train['ASI_category_encoded']\n",
    "\n",
    "# --- Train XGBoost model for feature importance ---\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=9,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# --- Extract feature importances ---\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# --- Display Top Features ---\n",
    "print(\"Most Important Features:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# # --- Plot Feature Importance ---\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(feature_importance_df[\"Feature\"][:20][::-1], feature_importance_df[\"Importance\"][:20][::-1])\n",
    "# plt.xlabel(\"Importance Score\")\n",
    "# plt.ylabel(\"Feature\")\n",
    "# plt.title(\"Top 20 Feature Importances (XGBoost)\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f7becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selected 47 features (covering 97% importance):\n",
      "['Temp_Mean', 'Temp_Range_Impact', 'Effective_Radiation', 'Temperature', 'Radiation_to_Temp', 'Radiation', 'Sunshine_Duration', 'Soil_Temperature', 'Dew_Point', 'Humidity_Saturated_Index', 'Temperature_Delta', 'Daylight_Duration', 'Temp_Diff_Air_Soil', 'Sunshine_Ratio', 'Daylight_Efficiency', 'Precipitation_Type_Snow', 'Temperature_Inversion_Delta', 'Soil_Temp_Air_Temp_Ratio', 'Precipitation', 'Soil_Moisture_RH_Interaction', 'Soil_Moisture', 'Radiation_Efficiency', 'Cloud_Rain_Interaction', 'Humidity_Moisture', 'Rainfall', 'Relative_Humidity', 'Radiation_Per_Hour', 'Snowfall', 'Temp_Humidity_Index', 'Rainfall_Intensity', 'Precipitation_Ratio_Rain', 'Precipitation_Hours', 'Cloud_Cover_Inverse', 'Pressure_Humidity_Interaction', 'Pressure_MSL', 'Wind_Speed', 'Humidity_to_Pressure', 'Cloud_Cover', 'Weather_Code', 'ID', 'Wind_Gust_Ratio', 'Surface_Pressure', 'Wind_Gusts', 'Wind_Ratio', 'Wind_Direction_Cos', 'Wind_Direction_Sin', 'Wind_Force_Index']\n",
      "\n",
      "üóëÔ∏è Dropped 7 less important features:\n",
      "['Wind_Intensity', 'Wind_Direction', 'Total_Precip', 'Pressure_MSL_Difference', 'Wind_Stress', 'Gust_Delta_Ratio', 'Pressure_MSL_Trend']\n"
     ]
    }
   ],
   "source": [
    "# --- Compute cumulative importance ---\n",
    "feature_importance_df = feature_importance_df.reset_index(drop=True)\n",
    "feature_importance_df[\"Cumulative_Importance\"] = feature_importance_df[\"Importance\"].cumsum()\n",
    "\n",
    "# --- Select features covering 97% of importance ---\n",
    "selected_features = feature_importance_df[feature_importance_df[\"Cumulative_Importance\"] <= 0.97][\"Feature\"].tolist()\n",
    "\n",
    "# --- Identify dropped features ---\n",
    "dropped_features = feature_importance_df[feature_importance_df[\"Cumulative_Importance\"] > 0.97][\"Feature\"].tolist()\n",
    "\n",
    "print(f\"‚úÖ Selected {len(selected_features)} features (covering 97% importance):\")\n",
    "print(selected_features)\n",
    "\n",
    "print(f\"\\nüóëÔ∏è Dropped {len(dropped_features)} less important features:\")\n",
    "print(dropped_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc03800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Split Complete:\n",
      "Training Shape   : (16337, 47)\n",
      "Validation Shape : (1816, 47)\n",
      "Target Distribution (Train):\n",
      "ASI_category_encoded\n",
      "1    0.701108\n",
      "0    0.172859\n",
      "2    0.126033\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Use selected features from the previous step ---\n",
    "X = df_train[selected_features]\n",
    "y = df_train[\"ASI_category_encoded\"]\n",
    "\n",
    "# --- Split the data ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.1,          # 20% for validation\n",
    "    stratify=y,             # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data Split Complete:\")\n",
    "print(f\"Training Shape   : {X_train.shape}\")\n",
    "print(f\"Validation Shape : {X_val.shape}\")\n",
    "print(f\"Target Distribution (Train):\")\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9186a2f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_es \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m      6\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,\n\u001b[0;32m      7\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# --- Train with early stopping (new syntax for XGBoost ‚â•2.0) ---\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m xgb_es\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     23\u001b[0m     X_train, y_train,\n\u001b[0;32m     24\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_train, y_train), (X_val, y_val)],\n\u001b[0;32m     25\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     26\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# ‚úÖ still works for backward compatibility\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# --- Best iteration ---\u001b[39;00m\n\u001b[0;32m     30\u001b[0m best_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(xgb_es, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, classification_report\n",
    "\n",
    "# --- Define model ---\n",
    "xgb_es = XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=1.0,\n",
    "    min_child_weight=4,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=3.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# --- Train with early stopping (new syntax for XGBoost ‚â•2.0) ---\n",
    "xgb_es.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=50  # ‚úÖ still works for backward compatibility\n",
    ")\n",
    "\n",
    "# --- Best iteration ---\n",
    "best_iter = getattr(xgb_es, \"best_iteration\", None)\n",
    "print(f\"\\n‚úÖ Best iteration (n_estimators used): {best_iter}\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = xgb_es.predict(X_train)\n",
    "val_preds = xgb_es.predict(X_val)\n",
    "train_probs = xgb_es.predict_proba(X_train)\n",
    "val_probs = xgb_es.predict_proba(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "val_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "train_logloss = log_loss(y_train, train_probs)\n",
    "val_logloss = log_loss(y_val, val_probs)\n",
    "\n",
    "print(\"\\n‚úÖ XGBoost Model Performance (with Early Stopping):\")\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Training F1 Score  : {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Training Log Loss  : {train_logloss:.4f}\")\n",
    "print(f\"Validation Log Loss: {val_logloss:.4f}\")\n",
    "print(f\"Œî F1 Gap           : {abs(train_f1 - val_f1):.4f}\")\n",
    "print(f\"Œî LogLoss Gap      : {abs(train_logloss - val_logloss):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(y_val, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9ba07de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Stacking Ensemble Performance:\n",
      "Training Accuracy : 0.9553\n",
      "Validation Accuracy: 0.9356\n",
      "Training F1 Score  : 0.9379\n",
      "Validation F1 Score: 0.9098\n",
      "Training Log Loss  : 0.1380\n",
      "Validation Log Loss: 0.1895\n",
      "Œî F1 Gap           : 0.0281\n",
      "Œî LogLoss Gap      : 0.0514\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       628\n",
      "           1       0.95      0.97      0.96      2546\n",
      "           2       0.91      0.86      0.89       457\n",
      "\n",
      "    accuracy                           0.94      3631\n",
      "   macro avg       0.92      0.90      0.91      3631\n",
      "weighted avg       0.94      0.94      0.94      3631\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, val_preds))\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# --- Train Ensemble ---\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# --- Evaluate ---\u001b[39;00m\n\u001b[0;32m    104\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m stacking_model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:717\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    213\u001b[0m         delayed(_fit_single_estimator)(\n\u001b[0;32m    214\u001b[0m             clone(est), X, y, routed_params[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    215\u001b[0m         )\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, all_estimators)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [Stacking Ensemble Model - Enhanced]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, classification_report\n",
    "\n",
    "# --- Define Base Models (using tuned params if available) ---\n",
    "base_models = [\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=109,\n",
    "        learning_rate=0.0504,\n",
    "        max_depth=4,\n",
    "        subsample=0.7587,\n",
    "        colsample_bytree=0.8293,\n",
    "        gamma=0.4669,\n",
    "        min_child_weight=5,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"lgbm\", LGBMClassifier(\n",
    "        n_estimators=103,\n",
    "        learning_rate=0.0158,\n",
    "        max_depth=7,\n",
    "        subsample=0.9337,\n",
    "        colsample_bytree=0.9123,\n",
    "        min_child_samples=73,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"cat\", CatBoostClassifier(\n",
    "        iterations=227,\n",
    "        learning_rate=0.0557,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=4.52,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=747,\n",
    "        max_depth=9,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# --- Define Meta (Blender) Model ---\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Build the Stacking Model ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "val_preds = stacking_model.predict(X_val)\n",
    "train_probs = stacking_model.predict_proba(X_train)\n",
    "val_probs = stacking_model.predict_proba(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "val_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "train_logloss = log_loss(y_train, train_probs)\n",
    "val_logloss = log_loss(y_val, val_probs)\n",
    "\n",
    "print(\"\\n Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Training F1 Score  : {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Training Log Loss  : {train_logloss:.4f}\")\n",
    "print(f\"Validation Log Loss: {val_logloss:.4f}\")\n",
    "print(f\"Œî F1 Gap           : {abs(train_f1 - val_f1):.4f}\")\n",
    "print(f\"Œî LogLoss Gap      : {abs(train_logloss - val_logloss):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(y_val, val_preds))\n",
    "\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "test_preds = stacking_model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy     : {test_acc:.4f}\")\n",
    "print(f\"Training F1 Score : {train_f1:.4f}\")\n",
    "print(f\"Test F1 Score     : {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7bdbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def tune_model_with_optuna(model_name, n_trials=30):\n",
    "    # --- Split train into sub-train/validation ---\n",
    "    X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    "    )\n",
    "\n",
    "    def objective(trial):\n",
    "        # XGBoost\n",
    "        if model_name == \"xgb\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0.0, 2.0),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 6),\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": 42,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                \"use_label_encoder\": False,\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": len(np.unique(y_train))\n",
    "            }\n",
    "            model = XGBClassifier(**params)\n",
    "\n",
    "        # LightGBM\n",
    "        elif model_name == \"lgbm\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1\n",
    "            }\n",
    "            model = LGBMClassifier(**params)\n",
    "\n",
    "        # CatBoost\n",
    "        elif model_name == \"cat\":\n",
    "            params = {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 7),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "                \"random_seed\": 42,\n",
    "                \"verbose\": 0\n",
    "            }\n",
    "            model = CatBoostClassifier(**params)\n",
    "\n",
    "        # Random Forest\n",
    "        elif model_name == \"rf\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 7),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "\n",
    "        # --- Train & Evaluate ---\n",
    "        model.fit(X_subtrain, y_subtrain)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        val_f1 = f1_score(y_val, val_preds, average=\"macro\")\n",
    "\n",
    "        # Objective: directly maximize F1\n",
    "        return val_f1\n",
    "\n",
    "    # --- Run Optuna study ---\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    # --- Results ---\n",
    "    print(f\"\\n‚úÖ Best {model_name.upper()} Params:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Best F1 Score: {study.best_value:.4f}\")\n",
    "\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e28a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:21,067] A new study created in memory with name: no-name-014aba8a-bae4-4e19-ba08-d854081061e8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc99f900cab463ab85086137505272e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:27,627] Trial 0 finished with value: 0.9170495113253788 and parameters: {'n_estimators': 250, 'learning_rate': 0.09556428757689246, 'max_depth': 6, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182, 'gamma': 0.3119890406724053, 'min_child_weight': 1}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:35,061] Trial 1 finished with value: 0.9158760157328456 and parameters: {'n_estimators': 447, 'learning_rate': 0.0641003510568888, 'max_depth': 6, 'subsample': 0.5102922471479012, 'colsample_bytree': 0.9849549260809971, 'gamma': 1.6648852816008435, 'min_child_weight': 2}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:38,287] Trial 2 finished with value: 0.905301111325278 and parameters: {'n_estimators': 172, 'learning_rate': 0.026506405886809047, 'max_depth': 4, 'subsample': 0.762378215816119, 'colsample_bytree': 0.7159725093210578, 'gamma': 0.5824582803960838, 'min_child_weight': 4}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:41,206] Trial 3 finished with value: 0.9069651726031239 and parameters: {'n_estimators': 155, 'learning_rate': 0.03629301836816964, 'max_depth': 4, 'subsample': 0.728034992108518, 'colsample_bytree': 0.8925879806965068, 'gamma': 0.39934756431671947, 'min_child_weight': 4}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:48,447] Trial 4 finished with value: 0.9124746197810897 and parameters: {'n_estimators': 337, 'learning_rate': 0.014180537144799797, 'max_depth': 6, 'subsample': 0.5852620618436457, 'colsample_bytree': 0.5325257964926398, 'gamma': 1.8977710745066665, 'min_child_weight': 6}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:56,331] Trial 5 finished with value: 0.913653300738651 and parameters: {'n_estimators': 424, 'learning_rate': 0.037415239225603365, 'max_depth': 3, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007, 'gamma': 0.24407646968955765, 'min_child_weight': 3}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:03:59,157] Trial 6 finished with value: 0.9152525381958855 and parameters: {'n_estimators': 113, 'learning_rate': 0.09183883618709039, 'max_depth': 4, 'subsample': 0.831261142176991, 'colsample_bytree': 0.6558555380447055, 'gamma': 1.0401360423556216, 'min_child_weight': 4}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:03:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:03,758] Trial 7 finished with value: 0.9135406748130906 and parameters: {'n_estimators': 174, 'learning_rate': 0.09726261649881028, 'max_depth': 6, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9474136752138245, 'gamma': 1.1957999576221703, 'min_child_weight': 6}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:06,066] Trial 8 finished with value: 0.8942433320959274 and parameters: {'n_estimators': 135, 'learning_rate': 0.027638457617723072, 'max_depth': 3, 'subsample': 0.6626651653816322, 'colsample_bytree': 0.6943386448447411, 'gamma': 0.5426980635477918, 'min_child_weight': 5}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:11,932] Trial 9 finished with value: 0.9167857178522567 and parameters: {'n_estimators': 243, 'learning_rate': 0.03528410587186427, 'max_depth': 5, 'subsample': 0.5704621124873813, 'colsample_bytree': 0.9010984903770198, 'gamma': 0.14910128735954165, 'min_child_weight': 6}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:17,577] Trial 10 finished with value: 0.9137099946810389 and parameters: {'n_estimators': 315, 'learning_rate': 0.07355202867601568, 'max_depth': 7, 'subsample': 0.9661451709558936, 'colsample_bytree': 0.5076838686640521, 'gamma': 0.7960866844403536, 'min_child_weight': 1}. Best is trial 0 with value: 0.9170495113253788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:23,569] Trial 11 finished with value: 0.9185590103473457 and parameters: {'n_estimators': 242, 'learning_rate': 0.05451798712798489, 'max_depth': 5, 'subsample': 0.6393145620149152, 'colsample_bytree': 0.8485779491700473, 'gamma': 0.04940924881227371, 'min_child_weight': 1}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:29,284] Trial 12 finished with value: 0.9150178952520784 and parameters: {'n_estimators': 237, 'learning_rate': 0.056386597372166704, 'max_depth': 5, 'subsample': 0.6779623244600864, 'colsample_bytree': 0.7950881832821013, 'gamma': 0.007463514646734178, 'min_child_weight': 1}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:38,944] Trial 13 finished with value: 0.914950839931814 and parameters: {'n_estimators': 238, 'learning_rate': 0.07944977289312445, 'max_depth': 7, 'subsample': 0.85058129769688, 'colsample_bytree': 0.8036067814232024, 'gamma': 0.006632540865966756, 'min_child_weight': 2}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:49,167] Trial 14 finished with value: 0.9155406008819992 and parameters: {'n_estimators': 361, 'learning_rate': 0.051088902098586644, 'max_depth': 6, 'subsample': 0.7573861576859734, 'colsample_bytree': 0.6044566965112071, 'gamma': 1.3268128572239652, 'min_child_weight': 2}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:04:55,475] Trial 15 finished with value: 0.9157143583493857 and parameters: {'n_estimators': 260, 'learning_rate': 0.07568513465785312, 'max_depth': 5, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.8091883372850146, 'gamma': 0.7347577744400322, 'min_child_weight': 1}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:04:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:06,292] Trial 16 finished with value: 0.9150679677162845 and parameters: {'n_estimators': 284, 'learning_rate': 0.04883308238585699, 'max_depth': 7, 'subsample': 0.6924745509172271, 'colsample_bytree': 0.596609562772596, 'gamma': 0.34173833856231534, 'min_child_weight': 3}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:14,636] Trial 17 finished with value: 0.9143662904394612 and parameters: {'n_estimators': 382, 'learning_rate': 0.08719593104665169, 'max_depth': 6, 'subsample': 0.6213946636000343, 'colsample_bytree': 0.775363380915974, 'gamma': 0.8337443690283718, 'min_child_weight': 2}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:19,766] Trial 18 finished with value: 0.9167994896858834 and parameters: {'n_estimators': 204, 'learning_rate': 0.06633981323438215, 'max_depth': 5, 'subsample': 0.791412280667512, 'colsample_bytree': 0.8609727402803514, 'gamma': 0.5369581152388117, 'min_child_weight': 1}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:25,789] Trial 19 finished with value: 0.9172077493871348 and parameters: {'n_estimators': 294, 'learning_rate': 0.062138220164854104, 'max_depth': 5, 'subsample': 0.9018049454302455, 'colsample_bytree': 0.5883435592119127, 'gamma': 0.24460496092909467, 'min_child_weight': 3}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:31,865] Trial 20 finished with value: 0.9180553805566144 and parameters: {'n_estimators': 494, 'learning_rate': 0.058703450391223405, 'max_depth': 4, 'subsample': 0.8979338809467865, 'colsample_bytree': 0.6455326383677563, 'gamma': 1.485151668905378, 'min_child_weight': 3}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:38,604] Trial 21 finished with value: 0.9166321749188838 and parameters: {'n_estimators': 483, 'learning_rate': 0.060184053781853285, 'max_depth': 4, 'subsample': 0.9097877650610291, 'colsample_bytree': 0.6477234193807914, 'gamma': 1.44396763782538, 'min_child_weight': 3}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:44,109] Trial 22 finished with value: 0.912970965126144 and parameters: {'n_estimators': 299, 'learning_rate': 0.04691983803303788, 'max_depth': 5, 'subsample': 0.9061080132164645, 'colsample_bytree': 0.6451371648959269, 'gamma': 1.6310394253424867, 'min_child_weight': 5}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:50,172] Trial 23 finished with value: 0.9156086895189731 and parameters: {'n_estimators': 389, 'learning_rate': 0.07057555087970209, 'max_depth': 4, 'subsample': 0.9989244219096844, 'colsample_bytree': 0.5497267584916317, 'gamma': 1.1051408114936678, 'min_child_weight': 3}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:05:57,076] Trial 24 finished with value: 0.9129500045727156 and parameters: {'n_estimators': 498, 'learning_rate': 0.0443380697648999, 'max_depth': 5, 'subsample': 0.88372204022497, 'colsample_bytree': 0.8566298325921834, 'gamma': 1.9468582820569833, 'min_child_weight': 5}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:05:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:06:00,124] Trial 25 finished with value: 0.9099641174669201 and parameters: {'n_estimators': 197, 'learning_rate': 0.05519621491059125, 'max_depth': 3, 'subsample': 0.9357575045838521, 'colsample_bytree': 0.7495815302113988, 'gamma': 0.15478316428385108, 'min_child_weight': 2}. Best is trial 11 with value: 0.9185590103473457.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:06:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:06:05,087] Trial 26 finished with value: 0.9200108436344022 and parameters: {'n_estimators': 333, 'learning_rate': 0.06376514010958909, 'max_depth': 4, 'subsample': 0.7105576225418185, 'colsample_bytree': 0.6803222620640019, 'gamma': 1.4554751073424212, 'min_child_weight': 3}. Best is trial 26 with value: 0.9200108436344022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:06:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:06:10,940] Trial 27 finished with value: 0.9177248371986106 and parameters: {'n_estimators': 436, 'learning_rate': 0.08583201625404041, 'max_depth': 4, 'subsample': 0.6285836974809043, 'colsample_bytree': 0.6736701896523487, 'gamma': 1.5146295322867214, 'min_child_weight': 4}. Best is trial 26 with value: 0.9200108436344022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:06:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:06:16,734] Trial 28 finished with value: 0.913451878606895 and parameters: {'n_estimators': 402, 'learning_rate': 0.04301447867794913, 'max_depth': 3, 'subsample': 0.722941617844659, 'colsample_bytree': 0.7512979163480473, 'gamma': 1.284535246652175, 'min_child_weight': 2}. Best is trial 26 with value: 0.9200108436344022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:06:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:06:21,579] Trial 29 finished with value: 0.9205029567214768 and parameters: {'n_estimators': 335, 'learning_rate': 0.06869744187596126, 'max_depth': 4, 'subsample': 0.5268577734910248, 'colsample_bytree': 0.6233187244227066, 'gamma': 1.7743397533828527, 'min_child_weight': 3}. Best is trial 29 with value: 0.9205029567214768.\n",
      "\n",
      "‚úÖ Best XGB Params:\n",
      "{'n_estimators': 335, 'learning_rate': 0.06869744187596126, 'max_depth': 4, 'subsample': 0.5268577734910248, 'colsample_bytree': 0.6233187244227066, 'gamma': 1.7743397533828527, 'min_child_weight': 3}\n",
      "Best F1 Score: 0.9205\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "# xgb_study = tune_model_with_optuna(\"xgb\", n_trials=30) considerable\n",
    "\n",
    "# LightGBM\n",
    "# lgbm_study = tune_model_with_optuna(\"lgbm\", n_trials=30) considerable \n",
    "\n",
    "# # CatBoost\n",
    "# cat_study = tune_model_with_optuna(\"cat\", n_trials=30)\n",
    "\n",
    "# # Random Forest\n",
    "# rf_study = tune_model_with_optuna(\"rf\", n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Best XGB Params:\n",
    "# {'n_estimators': 600, 'learning_rate': 0.09905399158869002, 'max_depth': 6, 'subsample': 0.6146009721997969, 'colsample_bytree': 0.9828472877209867, 'gamma': 1.5294297733838427, 'min_child_weight': 1}\n",
    "# Best Objective Value: 0.0686\n",
    "\n",
    "# ‚úÖ Best LGBM Params:\n",
    "# {'n_estimators': 316, 'learning_rate': 0.06533353663762796, 'max_depth': 7, 'subsample': 0.569746930326021, 'colsample_bytree': 0.6460723242676091, 'min_child_samples': 43}\n",
    "# Best Objective Value: 0.0701\n",
    "\n",
    "# ‚úÖ Best CAT Params:\n",
    "# {'iterations': 496, 'learning_rate': 0.055895437266264544, 'depth': 7, 'l2_leaf_reg': 8.50438821157523}\n",
    "# Best Objective Value: 0.0693\n",
    "\n",
    "# ‚úÖ Best RF Params:\n",
    "# {'n_estimators': 414, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
    "# Best Objective Value: 0.0732\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81fea224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Stacking Ensemble Performance:\n",
      "Training Accuracy : 0.9756\n",
      "Validation Accuracy: 0.9361\n",
      "Training F1 Score  : 0.9659\n",
      "Validation F1 Score: 0.9098\n",
      "Œî F1 Gap           : 0.0561\n"
     ]
    }
   ],
   "source": [
    "# %% [Stacking Ensemble Model - Tuned Base Models]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Define Tuned Base Models ---\n",
    "base_models = [\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.09905399158869002,\n",
    "        max_depth=6,\n",
    "        subsample=0.6146009721997969,\n",
    "        colsample_bytree=0.9828472877209867,\n",
    "        gamma=1.5294297733838427,\n",
    "        min_child_weight=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        objective=\"multi:softprob\",\n",
    "        use_label_encoder=False,\n",
    "        num_class=len(np.unique(y_train))\n",
    "    )),\n",
    "    (\"lgbm\", LGBMClassifier(\n",
    "        n_estimators=316,\n",
    "        learning_rate=0.06533353663762796,\n",
    "        max_depth=7,\n",
    "        subsample=0.569746930326021,\n",
    "        colsample_bytree=0.6460723242676091,\n",
    "        min_child_samples=43,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"cat\", CatBoostClassifier(\n",
    "        iterations=496,\n",
    "        learning_rate=0.055895437266264544,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=8.50438821157523,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=414,\n",
    "        max_depth=12,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='log2',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# --- Meta Model (Blender) ---\n",
    "meta_model = LogisticRegression(\n",
    "    C=0.3,\n",
    "    max_iter=1000,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Build Stacking Ensemble ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "val_preds = stacking_model.predict(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "val_f1 = f1_score(y_val, val_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n‚úÖ Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Training F1 Score  : {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Œî F1 Gap           : {abs(train_f1 - val_f1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae135eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Stacking Ensemble Performance (Ridge Meta-Model):\n",
      "Training Accuracy : 0.9861\n",
      "Validation Accuracy: 0.9383\n",
      "Training F1 Score  : 0.9805\n",
      "Validation F1 Score: 0.9145\n",
      "Œî F1 Gap           : 0.0661\n"
     ]
    }
   ],
   "source": [
    "# %% [Stacking Ensemble Model - Tuned Base Models with Ridge Meta-Model]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Define Tuned Base Models ---\n",
    "base_models = [\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.09905399158869002,\n",
    "        max_depth=6,\n",
    "        subsample=0.6146009721997969,\n",
    "        colsample_bytree=0.9828472877209867,\n",
    "        gamma=1.5294297733838427,\n",
    "        min_child_weight=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        objective=\"multi:softprob\",\n",
    "        use_label_encoder=False,\n",
    "        num_class=len(np.unique(y_train))\n",
    "    )),\n",
    "    # (\"lgbm\", LGBMClassifier(\n",
    "    #     n_estimators=316,\n",
    "    #     learning_rate=0.06533353663762796,\n",
    "    #     max_depth=7,\n",
    "    #     subsample=0.569746930326021,\n",
    "    #     colsample_bytree=0.6460723242676091,\n",
    "    #     min_child_samples=43,\n",
    "    #     random_state=42,\n",
    "    #     n_jobs=-1\n",
    "    # )),\n",
    "    # (\"cat\", CatBoostClassifier(\n",
    "    #     iterations=496,\n",
    "    #     learning_rate=0.055895437266264544,\n",
    "    #     depth=7,\n",
    "    #     l2_leaf_reg=8.50438821157523,\n",
    "    #     verbose=0,\n",
    "    #     random_seed=42\n",
    "    # )),\n",
    "    # (\"rf\", RandomForestClassifier(\n",
    "    #     n_estimators=414,\n",
    "    #     max_depth=12,\n",
    "    #     min_samples_split=6,\n",
    "    #     min_samples_leaf=2,\n",
    "    #     max_features='log2',\n",
    "    #     random_state=42,\n",
    "    #     n_jobs=-1\n",
    "    # ))\n",
    "]\n",
    "\n",
    "# --- Meta Model (Ridge Classifier with CV) ---\n",
    "meta_model = RidgeClassifierCV(\n",
    "    alphas=np.logspace(-3, 3, 10),  # range of regularization strengths\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# --- Build Stacking Ensemble ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "val_preds = stacking_model.predict(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "val_f1 = f1_score(y_val, val_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n‚úÖ Stacking Ensemble Performance (Ridge Meta-Model):\")\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Training F1 Score  : {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Œî F1 Gap           : {abs(train_f1 - val_f1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a697b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
