{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3119ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (18153, 21)\n",
      "Test shape: (7780, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ASI_category</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>Soil_Temperature</th>\n",
       "      <th>Radiation</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Wind_Gusts</th>\n",
       "      <th>...</th>\n",
       "      <th>Surface_Pressure</th>\n",
       "      <th>Relative_Humidity</th>\n",
       "      <th>Soil_Moisture</th>\n",
       "      <th>Dew_Point</th>\n",
       "      <th>Sunshine_Duration</th>\n",
       "      <th>Cloud_Cover</th>\n",
       "      <th>Precipitation_Hours</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Weather_Code</th>\n",
       "      <th>Daylight_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19554</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.931231</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.757673</td>\n",
       "      <td>0.879671</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538056</td>\n",
       "      <td>55</td>\n",
       "      <td>0.546243</td>\n",
       "      <td>17.564597</td>\n",
       "      <td>53252.08</td>\n",
       "      <td>12.136192</td>\n",
       "      <td>1</td>\n",
       "      <td>176.459082</td>\n",
       "      <td>51</td>\n",
       "      <td>58772.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25205</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.566323</td>\n",
       "      <td>0.096715</td>\n",
       "      <td>0.096715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291448</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.532172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568475</td>\n",
       "      <td>88</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>5.692134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>91.901341</td>\n",
       "      <td>16</td>\n",
       "      <td>232.433005</td>\n",
       "      <td>61</td>\n",
       "      <td>28143.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771</td>\n",
       "      <td>Poor</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277340</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>0.189008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706520</td>\n",
       "      <td>78</td>\n",
       "      <td>0.791908</td>\n",
       "      <td>-25.264420</td>\n",
       "      <td>30213.79</td>\n",
       "      <td>18.859670</td>\n",
       "      <td>0</td>\n",
       "      <td>44.688600</td>\n",
       "      <td>3</td>\n",
       "      <td>34621.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635669</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.134048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>57</td>\n",
       "      <td>0.473988</td>\n",
       "      <td>5.913865</td>\n",
       "      <td>44627.21</td>\n",
       "      <td>38.759757</td>\n",
       "      <td>0</td>\n",
       "      <td>333.640418</td>\n",
       "      <td>3</td>\n",
       "      <td>59192.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14036</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.827170</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743855</td>\n",
       "      <td>0.781282</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.391421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546378</td>\n",
       "      <td>50</td>\n",
       "      <td>0.459538</td>\n",
       "      <td>9.661455</td>\n",
       "      <td>45267.17</td>\n",
       "      <td>60.058955</td>\n",
       "      <td>1</td>\n",
       "      <td>86.996954</td>\n",
       "      <td>51</td>\n",
       "      <td>59956.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID ASI_category  Temperature  Precipitation  Rainfall  Snowfall  \\\n",
       "0  19554     Moderate     0.931231       0.000912  0.000912       0.0   \n",
       "1  25205     Moderate     0.566323       0.096715  0.096715       0.0   \n",
       "2    771         Poor     0.018033       0.000000  0.000000       0.0   \n",
       "3   1976         Good     0.717541       0.000000  0.000000       0.0   \n",
       "4  14036     Moderate     0.827170       0.001825  0.001825       0.0   \n",
       "\n",
       "   Soil_Temperature  Radiation  Wind_Speed  Wind_Gusts  ...  Surface_Pressure  \\\n",
       "0          0.757673   0.879671    0.179293    0.193029  ...          0.538056   \n",
       "1          0.291448   0.008913    0.588384    0.532172  ...          0.568475   \n",
       "2          0.000000   0.277340    0.247475    0.189008  ...          0.706520   \n",
       "3          0.635669   0.796709    0.123737    0.134048  ...          0.547500   \n",
       "4          0.743855   0.781282    0.343434    0.391421  ...          0.546378   \n",
       "\n",
       "   Relative_Humidity  Soil_Moisture  Dew_Point  Sunshine_Duration  \\\n",
       "0                 55       0.546243  17.564597           53252.08   \n",
       "1                 88       0.557803   5.692134               0.00   \n",
       "2                 78       0.791908 -25.264420           30213.79   \n",
       "3                 57       0.473988   5.913865           44627.21   \n",
       "4                 50       0.459538   9.661455           45267.17   \n",
       "\n",
       "   Cloud_Cover  Precipitation_Hours  Wind_Direction  Weather_Code  \\\n",
       "0    12.136192                    1      176.459082            51   \n",
       "1    91.901341                   16      232.433005            61   \n",
       "2    18.859670                    0       44.688600             3   \n",
       "3    38.759757                    0      333.640418             3   \n",
       "4    60.058955                    1       86.996954            51   \n",
       "\n",
       "   Daylight_Duration  \n",
       "0           58772.52  \n",
       "1           28143.12  \n",
       "2           34621.43  \n",
       "3           59192.17  \n",
       "4           59956.03  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load train and test data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d6ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train['ASI_category_encoded'] = le.fit_transform(df_train['ASI_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a30a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    # --- Base features ---\n",
    "    df['Temp_Range_Impact'] = df['Temperature'] * df['Soil_Temperature']\n",
    "    df['Humidity_Moisture'] = df['Relative_Humidity'] * df['Soil_Moisture']\n",
    "    df['Effective_Radiation'] = df['Radiation'] * (df['Sunshine_Duration'] / (df['Daylight_Duration'] + 1e-6))\n",
    "    df['Total_Precip'] = df['Rainfall'] + df['Snowfall'] + df['Precipitation']\n",
    "    df['Wind_Intensity'] = df['Wind_Speed'] * df['Wind_Gusts']\n",
    "    df['Pressure_Humidity_Interaction'] = df['Surface_Pressure'] * df['Relative_Humidity']\n",
    "\n",
    "    # --- Extended interactions ---\n",
    "    df['Temp_Diff_Air_Soil'] = df['Temperature'] - df['Soil_Temperature']\n",
    "    df['Temp_Mean'] = (df['Temperature'] + df['Soil_Temperature']) / 2\n",
    "    df['Temp_Humidity_Index'] = df['Temperature'] * df['Relative_Humidity']\n",
    "    df['Radiation_Per_Hour'] = df['Radiation'] / (df['Sunshine_Duration'] + 1e-6)\n",
    "    df['Sunshine_Ratio'] = df['Sunshine_Duration'] / (df['Daylight_Duration'] + 1e-6)\n",
    "    df['Wind_Stress'] = df['Wind_Speed'] ** 2\n",
    "    df['Wind_Ratio'] = df['Wind_Gusts'] / (df['Wind_Speed'] + 1e-6)\n",
    "    df['Humidity_to_Pressure'] = df['Relative_Humidity'] / (df['Surface_Pressure'] + 1e-6)\n",
    "    df['Radiation_to_Temp'] = df['Radiation'] / (df['Temperature'] + 1e-6)\n",
    "\n",
    "    # --- Clean NaNs only for numeric columns ---\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    # --- Drop duplicate columns ---\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    return df\n",
    "df_train = feature_engineering(df_train)\n",
    "df_test = feature_engineering(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bc902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = df_train.select_dtypes(include=[np.number]).columns.drop('ASI_category_encoded')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "df_test[num_cols] = scaler.transform(df_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43a8931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 18\n",
      "Selected features: ['Temperature', 'Precipitation', 'Rainfall', 'Soil_Temperature', 'Radiation', 'Relative_Humidity', 'Soil_Moisture', 'Dew_Point', 'Sunshine_Duration', 'Daylight_Duration', 'Temp_Range_Impact', 'Humidity_Moisture', 'Effective_Radiation', 'Temp_Diff_Air_Soil', 'Temp_Mean', 'Temp_Humidity_Index', 'Sunshine_Ratio', 'Radiation_to_Temp']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X = df_train.drop(['ASI_category', 'ASI_category_encoded'], axis=1)\n",
    "y = df_train['ASI_category_encoded']\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X, y)\n",
    "selector = SelectFromModel(xgb_model, prefit=True, threshold='median')\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Number of selected features:\", len(selected_features))\n",
    "print(\"Selected features:\", list(selected_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffda7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "X_final = df_train[selected_features]\n",
    "y_final = df_train['ASI_category_encoded']\n",
    "X_test_final = df_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66169194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:23:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model Performance:\n",
      "Training Accuracy: 0.9406\n",
      "Test Accuracy     : 0.9281\n",
      "Training F1 Score : 0.9184\n",
      "Test F1 Score     : 0.9026\n"
     ]
    }
   ],
   "source": [
    "# %% [Train/Test Split + XGBoost Evaluation - Tuned Version]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- Split data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, stratify=y_final, random_state=42\n",
    ")\n",
    "\n",
    "# --- Define tuned model ---\n",
    "model = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    # subsample=0.5921834617441386,\n",
    "    # colsample_bytree=0.6340862288557617,\n",
    "    # gamma=1.399534210191034,\n",
    "    # min_child_weight=3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    # eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(np.unique(y_final))\n",
    ")\n",
    "\n",
    "# --- Train model ---\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Predictions ---\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "# --- Metrics ---\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nâœ… Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy     : {test_acc:.4f}\")\n",
    "print(f\"Training F1 Score : {train_f1:.4f}\")\n",
    "print(f\"Test F1 Score     : {test_f1:.4f}\")\n",
    "\n",
    "# print(\"\\nðŸ“Š Classification Report (Test):\")\n",
    "# print(classification_report(y_test, test_preds, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2af81153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [Train Final Model on Full Data and Create submission.csv]\n",
    "\n",
    "# import xgboost as xgb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # --- Compute average best iteration from CV ---\n",
    "# best_iter = int(np.mean(best_iterations))\n",
    "# print(f\"âœ… Using Best Iteration from CV: {best_iter}\")\n",
    "\n",
    "# # --- Prepare final training data ---\n",
    "# X_final = X_final.loc[:, ~X_final.columns.duplicated()]\n",
    "# y_final = y_final.astype(int)\n",
    "# X_final = X_final.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# # --- Convert to DMatrix ---\n",
    "# dtrain_full = xgb.DMatrix(X_final, label=y_final)\n",
    "\n",
    "# # --- Parameters ---\n",
    "# params = {\n",
    "#     \"objective\": \"multi:softmax\",\n",
    "#     \"num_class\": len(np.unique(y_final)),\n",
    "#     \"eval_metric\": \"mlogloss\",\n",
    "#     \"learning_rate\": 0.03,\n",
    "#     \"max_depth\": 8,\n",
    "#     \"subsample\": 0.7,\n",
    "#     \"colsample_bytree\": 0.7,\n",
    "#     \"gamma\": 1.2,\n",
    "#     \"reg_alpha\": 0.3,\n",
    "#     \"reg_lambda\": 1.0,\n",
    "#     \"min_child_weight\": 4,\n",
    "#     \"seed\": 42,\n",
    "#     \"nthread\": -1,\n",
    "# }\n",
    "\n",
    "# # --- Train final model on full data ---\n",
    "# final_model = xgb.train(\n",
    "#     params=params,\n",
    "#     dtrain=dtrain_full,\n",
    "#     num_boost_round=best_iter\n",
    "# )\n",
    "\n",
    "# # --- Prepare test set (use the already loaded one) ---\n",
    "# test_ids = df_test[\"ID\"].copy()\n",
    "\n",
    "# # --- Apply same feature engineering ---\n",
    "# df_test = feature_engineering(df_test)\n",
    "\n",
    "# # --- Scale numeric features ---\n",
    "# df_test[num_cols] = scaler.transform(df_test[num_cols])\n",
    "\n",
    "# # --- Select same features ---\n",
    "# X_test_final = df_test[selected_features].reindex(columns=X_final.columns, fill_value=0)\n",
    "\n",
    "# # --- Convert to DMatrix ---\n",
    "# dtest = xgb.DMatrix(X_test_final)\n",
    "\n",
    "# # --- Predict ---\n",
    "# test_preds = final_model.predict(dtest)\n",
    "\n",
    "# # --- Map predictions back to original labels ---\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": test_ids,\n",
    "#     \"ASI_category\": le.inverse_transform(test_preds.astype(int))\n",
    "# })\n",
    "\n",
    "# # --- Save submission ---\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"âœ… submission.csv created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "898cb08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Stacking Ensemble Performance:\n",
      "Training Accuracy: 0.9731\n",
      "Test Accuracy     : 0.9446\n",
      "Training F1 Score : 0.9622\n",
      "Test F1 Score     : 0.9245\n"
     ]
    }
   ],
   "source": [
    "# %% [Stacking Ensemble Model]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Define Base Models ---\n",
    "base_models = [\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"lgbm\", LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"cat\", CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# --- Define Meta Model (Blender) ---\n",
    "meta_model = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "\n",
    "# --- Build Stacking Ensemble ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "test_preds = stacking_model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\nâœ… Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy     : {test_acc:.4f}\")\n",
    "print(f\"Training F1 Score : {train_f1:.4f}\")\n",
    "print(f\"Test F1 Score     : {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6f36d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def tune_model_with_optuna(model_name, n_trials=30):\n",
    "    def objective(trial):\n",
    "        if model_name == \"xgb\":\n",
    "            params = {\n",
    "                # --- Core boosting parameters ---\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "                \n",
    "                # --- Subsampling for regularization ---\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "                \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "\n",
    "                # --- Regularization parameters ---\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),   # L2 regularization\n",
    "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),     # L1 regularization\n",
    "\n",
    "                # --- Tree construction method ---\n",
    "                \"tree_method\": trial.suggest_categorical(\"tree_method\", [\"hist\", \"approx\", \"auto\"]),\n",
    "                \n",
    "                # --- Booster type ---\n",
    "                \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "\n",
    "                # --- DART-specific (only applies if booster='dart') ---\n",
    "                \"sample_type\": trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "                \"normalize_type\": trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]),\n",
    "                \"rate_drop\": trial.suggest_float(\"rate_drop\", 0.0, 0.5),\n",
    "                \"skip_drop\": trial.suggest_float(\"skip_drop\", 0.0, 0.5),\n",
    "\n",
    "                # --- Class imbalance & optimization ---\n",
    "                \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 0.8, 2.0),\n",
    "                \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "\n",
    "                # --- Misc ---\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": 42,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"use_label_encoder\": False,\n",
    "                \"num_class\": len(np.unique(y_train)),\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "\n",
    "        elif model_name == \"lgbm\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1\n",
    "            }\n",
    "            model = LGBMClassifier(**params)\n",
    "\n",
    "        elif model_name == \"cat\":\n",
    "            params = {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 200, 800),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "                \"random_seed\": 42,\n",
    "                \"verbose\": 0\n",
    "            }\n",
    "            model = CatBoostClassifier(**params)\n",
    "\n",
    "        elif model_name == \"rf\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 800),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "\n",
    "        # --- Train & Evaluate ---\n",
    "        model.fit(X_train, y_train)\n",
    "        train_preds = model.predict(X_train)\n",
    "        test_preds = model.predict(X_test)\n",
    "\n",
    "        train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "        test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "        # Objective: minimize overfitting + bad generalization\n",
    "        f1_gap = abs(train_f1 - test_f1)\n",
    "        score = f1_gap + (1 - test_f1) * 0.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\nâœ… Best {model_name.upper()} Params:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Best Objective Value: {study.best_value:.4f}\")\n",
    "\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d1a06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# xgb_study = tune_model_with_optuna(\"xgb\", n_trials=30)\n",
    "\n",
    "# LightGBM\n",
    "# lgbm_study = tune_model_with_optuna(\"lgbm\", n_trials=30)\n",
    "\n",
    "# # CatBoost\n",
    "# cat_study = tune_model_with_optuna(\"cat\", n_trials=30)\n",
    "\n",
    "# # Random Forest\n",
    "# rf_study = tune_model_with_optuna(\"rf\", n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f1b3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best XGB Params:\n",
    "# {'n_estimators': 109, 'learning_rate': 0.05041730562003292, 'max_depth': 4, 'subsample': 0.7586762826070333, 'colsample_bytree': 0.8293400793920931, 'gamma': 0.46689011567137706, 'min_child_weight': 5}\n",
    "# Best Objective Value: 0.0560\n",
    "\n",
    "# Best LGBM Params:\n",
    "# {'n_estimators': 103, 'learning_rate': 0.015769130736262276, 'max_depth': 7, 'subsample': 0.9336953602399801, 'colsample_bytree': 0.9123161127609902, 'min_child_samples': 73}\n",
    "# Best Objective Value: 0.0605\n",
    "\n",
    "# Best CAT Params:\n",
    "# {'iterations': 227, 'learning_rate': 0.05573763155224416, 'depth': 7, 'l2_leaf_reg': 4.51792861553295}\n",
    "# Best Objective Value: 0.0448\n",
    "\n",
    "# Best RF Params:\n",
    "# {'n_estimators': 747, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': None}\n",
    "# Best Objective Value: 0.0672\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baa61f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Stacking Ensemble Performance:\n",
      "Training Accuracy: 0.9505\n",
      "Test Accuracy     : 0.9422\n",
      "Training F1 Score : 0.9305\n",
      "Test F1 Score     : 0.9204\n"
     ]
    }
   ],
   "source": [
    "# %% [Stacking Ensemble Model - Tuned Base Models]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# --- Define Tuned Base Models ---\n",
    "base_models = [\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=109,\n",
    "        learning_rate=0.05041730562003292,\n",
    "        max_depth=4,\n",
    "        subsample=0.7586762826070333,\n",
    "        colsample_bytree=0.8293400793920931,\n",
    "        gamma=0.46689011567137706,\n",
    "        min_child_weight=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        objective=\"multi:softprob\",\n",
    "        use_label_encoder=False,\n",
    "        num_class=len(np.unique(y_final))\n",
    "    )),\n",
    "    (\"lgbm\", LGBMClassifier(\n",
    "        n_estimators=103,\n",
    "        learning_rate=0.015769130736262276,\n",
    "        max_depth=7,\n",
    "        subsample=0.9336953602399801,\n",
    "        colsample_bytree=0.9123161127609902,\n",
    "        min_child_samples=73,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    (\"cat\", CatBoostClassifier(\n",
    "        iterations=227,\n",
    "        learning_rate=0.05573763155224416,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=4.51792861553295,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=747,\n",
    "        max_depth=9,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# --- Meta Model (Blender) ---\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Build Stacking Ensemble ---\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Ensemble ---\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "test_preds = stacking_model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\nâœ… Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy     : {test_acc:.4f}\")\n",
    "print(f\"Training F1 Score : {train_f1:.4f}\")\n",
    "print(f\"Test F1 Score     : {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c3b9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Stacking Ensemble Performance:\n",
      "Training Accuracy : 0.9505\n",
      "Test Accuracy     : 0.9422\n",
      "Training F1 Score : 0.9305\n",
      "Test F1 Score     : 0.9204\n",
      "Training Log Loss : 0.1499\n",
      "Test Log Loss     : 0.1676\n",
      "Î” F1 Gap          : 0.0101\n",
      "Î” LogLoss Gap     : 0.0177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# --- Evaluate ---\n",
    "train_preds = stacking_model.predict(X_train)\n",
    "test_preds = stacking_model.predict(X_test)\n",
    "\n",
    "# Also get class probabilities for log loss\n",
    "train_probs = stacking_model.predict_proba(X_train)\n",
    "test_probs = stacking_model.predict_proba(X_test)\n",
    "\n",
    "# --- Metrics ---\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average=\"macro\")\n",
    "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "\n",
    "# --- Log Loss ---\n",
    "train_logloss = log_loss(y_train, train_probs)\n",
    "test_logloss = log_loss(y_test, test_probs)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nâœ… Stacking Ensemble Performance:\")\n",
    "print(f\"Training Accuracy : {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy     : {test_acc:.4f}\")\n",
    "print(f\"Training F1 Score : {train_f1:.4f}\")\n",
    "print(f\"Test F1 Score     : {test_f1:.4f}\")\n",
    "print(f\"Training Log Loss : {train_logloss:.4f}\")\n",
    "print(f\"Test Log Loss     : {test_logloss:.4f}\")\n",
    "print(f\"Î” F1 Gap          : {abs(train_f1 - test_f1):.4f}\")\n",
    "print(f\"Î” LogLoss Gap     : {abs(train_logloss - test_logloss):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ade44d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training final stacking ensemble on the entire dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generating predictions on test set...\n",
      "\n",
      "âœ… Submission file created successfully: submission.csv\n",
      "      ID ASI_category\n",
      "0  15628         Good\n",
      "1   9358     Moderate\n",
      "2  12927     Moderate\n",
      "3  23980         Poor\n",
      "4   1032     Moderate\n"
     ]
    }
   ],
   "source": [
    "# %% [Train Final Stacking Ensemble on Full Data + Create Clean submission.csv]\n",
    "\n",
    "print(\"ðŸš€ Training final stacking ensemble on the entire dataset...\")\n",
    "\n",
    "# --- Retrain the stacking model on the full dataset ---\n",
    "stacking_model.fit(X_final, y_final)\n",
    "\n",
    "# --- Prepare clean test data ---\n",
    "test = pd.read_csv(\"test.csv\")  # reload to restore original IDs\n",
    "test_ids = test[\"ID\"].copy()    # keep original IDs\n",
    "\n",
    "# Reapply feature engineering & scaling (without touching ID)\n",
    "df_test_fe = feature_engineering(test)\n",
    "df_test_fe[num_cols] = scaler.transform(df_test_fe[num_cols])\n",
    "X_test_final = df_test_fe[selected_features].reindex(columns=X_final.columns, fill_value=0)\n",
    "\n",
    "# --- Predict on test set ---\n",
    "print(\"ðŸ§  Generating predictions on test set...\")\n",
    "final_preds = stacking_model.predict(X_test_final)\n",
    "\n",
    "# --- Decode numeric predictions back to original labels ---\n",
    "final_preds = le.inverse_transform(final_preds)\n",
    "\n",
    "# --- Create submission DataFrame ---\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"ASI_category\": final_preds\n",
    "})\n",
    "\n",
    "# --- Save submission file ---\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nâœ… Submission file created successfully: submission.csv\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76880e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
