{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df78413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Imports & Setup\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from itertools import product\n",
    "from scipy.optimize import minimize\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "results = {}  # üß† to store all F1 results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61bdb4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data: (14522, 19) | Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# %% [2] Data Loading\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.drop(columns=[\"ASI_category\", \"ID\"])\n",
    "y = df[\"ASI_category\"].astype('category').cat.codes\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"‚úÖ Data: {X_train.shape} | Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef5bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3847\n",
      "[LightGBM] [Info] Number of data points in the train set: 14522, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.755382\n",
      "[LightGBM] [Info] Start training from score -0.355142\n",
      "[LightGBM] [Info] Start training from score -2.070802\n",
      "‚úÖ Base models trained.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=600, max_depth=8, learning_rate=0.05, random_state=42)\n",
    "lgb = LGBMClassifier(n_estimators=400, max_depth=12, learning_rate=0.06, random_state=42)\n",
    "rf  = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=42)\n",
    "\n",
    "for model in [xgb, lgb, rf]:\n",
    "    model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Base models trained.\")\n",
    "\n",
    "# Base model F1s\n",
    "results[\"XGBoost\"] = f1_score(y_val, xgb.predict(X_val), average=\"macro\")\n",
    "results[\"LightGBM\"] = f1_score(y_val, lgb.predict(X_val), average=\"macro\")\n",
    "results[\"RandomForest\"] = f1_score(y_val, rf.predict(X_val), average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac7e6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Weighted Avg F1=0.9224, Weights=(np.float64(0.5), np.float64(0.30000000000000004), np.float64(0.9))\n"
     ]
    }
   ],
   "source": [
    "# %% [4] Weighted Blending\n",
    "xgb_p, lgb_p, rf_p = xgb.predict_proba(X_val), lgb.predict_proba(X_val), rf.predict_proba(X_val)\n",
    "grid = np.arange(0.1, 1.1, 0.1); best_f1, best_w = 0, None\n",
    "for w1, w2, w3 in product(grid, repeat=3):\n",
    "    probs = (w1*xgb_p + w2*lgb_p + w3*rf_p) / (w1+w2+w3)\n",
    "    f1 = f1_score(y_val, np.argmax(probs, axis=1), average=\"macro\")\n",
    "    if f1 > best_f1: best_f1, best_w = f1, (w1, w2, w3)\n",
    "results[\"Weighted Ensemble\"] = best_f1\n",
    "print(f\"üî• Weighted Avg F1={best_f1:.4f}, Weights={best_w}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6bb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3822\n",
      "[LightGBM] [Info] Number of data points in the train set: 11617, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.764838\n",
      "[LightGBM] [Info] Start training from score -0.354574\n",
      "[LightGBM] [Info] Start training from score -2.061103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 11617, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.743449\n",
      "[LightGBM] [Info] Start training from score -0.360482\n",
      "[LightGBM] [Info] Start training from score -2.057728\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3822\n",
      "[LightGBM] [Info] Number of data points in the train set: 11618, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.762413\n",
      "[LightGBM] [Info] Start training from score -0.352576\n",
      "[LightGBM] [Info] Start training from score -2.075490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3811\n",
      "[LightGBM] [Info] Number of data points in the train set: 11618, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.739606\n",
      "[LightGBM] [Info] Start training from score -0.354170\n",
      "[LightGBM] [Info] Start training from score -2.098384\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3819\n",
      "[LightGBM] [Info] Number of data points in the train set: 11618, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.766937\n",
      "[LightGBM] [Info] Start training from score -0.353924\n",
      "[LightGBM] [Info] Start training from score -2.061866\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros((len(X_train), len(np.unique(y))))\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    for model in [XGBClassifier(), LGBMClassifier(), RandomForestClassifier()]:\n",
    "        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "        oof_preds[val_idx] += model.predict_proba(X_train.iloc[val_idx])\n",
    "oof_preds /= 3\n",
    "meta_lr = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_lr.fit(oof_preds, y_train)\n",
    "stack_preds = meta_lr.predict(np.mean([xgb_p, lgb_p, rf_p], axis=0))\n",
    "results[\"OOF Stacking\"] = f1_score(y_val, stack_preds, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75403d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Pseudo-labels added (3096), F1=0.5532\n"
     ]
    }
   ],
   "source": [
    "meta_probs = meta_lr.predict_proba(np.mean([xgb_p, lgb_p, rf_p], axis=0))\n",
    "conf = np.max(meta_probs, axis=1)\n",
    "pseudo_idx = np.where(conf >= 0.95)[0]\n",
    "if len(pseudo_idx):\n",
    "    X_aug = pd.concat([X_train, X_val.iloc[pseudo_idx]])\n",
    "    y_aug = np.concatenate([y_train, y_val.iloc[pseudo_idx]])\n",
    "    meta_lr.fit(X_aug[X_train.columns], y_aug)\n",
    "    pseudo_f1 = f1_score(y_val, meta_lr.predict(X_val), average=\"macro\")\n",
    "    results[\"Pseudo-Labeling\"] = pseudo_f1\n",
    "    print(f\"üöÄ Pseudo-labels added ({len(pseudo_idx)}), F1={pseudo_f1:.4f}\")\n",
    "else:\n",
    "    results[\"Pseudo-Labeling\"] = results[\"OOF Stacking\"]\n",
    "    print(\"‚ö†Ô∏è No pseudo-labels confident enough.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60882a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "üß† Neural Stacker F1: 0.9231923585751202\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.keras import models, layers\n",
    "    X_meta = np.hstack([xgb_p, lgb_p, rf_p])\n",
    "    meta_nn = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_meta.shape[1],)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "    ])\n",
    "    meta_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    meta_nn.fit(X_meta, y_val, epochs=30, batch_size=64, verbose=0)\n",
    "    nn_preds = np.argmax(meta_nn.predict(X_meta), axis=1)\n",
    "    results[\"Neural Stacker\"] = f1_score(y_val, nn_preds, average='macro')\n",
    "    print(\"üß† Neural Stacker F1:\", results[\"Neural Stacker\"])\n",
    "except Exception as e:\n",
    "    results[\"Neural Stacker\"] = np.nan\n",
    "    print(\"üí§ Skipped NN stacking:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7a3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [8] Bayesian Averaging\n",
    "def objective(w):\n",
    "    w = np.clip(w, 0, 1)\n",
    "    p = (w[0]*xgb_p + w[1]*lgb_p + w[2]*rf_p) / np.sum(w)\n",
    "    return -f1_score(y_val, np.argmax(p, axis=1), average=\"macro\")\n",
    "\n",
    "res = minimize(objective, [0.33,0.33,0.33], bounds=[(0,1)]*3)\n",
    "opt_w = res.x\n",
    "p = (opt_w[0]*xgb_p + opt_w[1]*lgb_p + opt_w[2]*rf_p)/np.sum(opt_w)\n",
    "results[\"Bayesian Weighted\"] = f1_score(y_val, np.argmax(p,axis=1),average='macro')\n",
    "\n",
    "# %% [9] Knowledge Distillation\n",
    "teacher_probs = (xgb_p + lgb_p + rf_p) / 3\n",
    "student = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
    "student.fit(X_val, np.argmax(teacher_probs, axis=1))\n",
    "results[\"Distilled Student\"] = f1_score(y_val, student.predict(X_val), average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe8608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Ensemble Leaderboard:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_95f6c_row0_col0, #T_95f6c_row1_col0, #T_95f6c_row2_col0 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95f6c_row3_col0 {\n",
       "  background-color: #fbe723;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95f6c_row4_col0 {\n",
       "  background-color: #f8e621;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95f6c_row5_col0 {\n",
       "  background-color: #f6e620;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95f6c_row6_col0 {\n",
       "  background-color: #d5e21a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_95f6c_row7_col0 {\n",
       "  background-color: #460a5d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_95f6c_row8_col0 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_95f6c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_95f6c_level0_col0\" class=\"col_heading level0 col0\" >F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row0\" class=\"row_heading level0 row0\" >Neural Stacker</th>\n",
       "      <td id=\"T_95f6c_row0_col0\" class=\"data row0 col0\" >0.923192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row1\" class=\"row_heading level0 row1\" >Weighted Ensemble</th>\n",
       "      <td id=\"T_95f6c_row1_col0\" class=\"data row1 col0\" >0.922397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row2\" class=\"row_heading level0 row2\" >LightGBM</th>\n",
       "      <td id=\"T_95f6c_row2_col0\" class=\"data row2 col0\" >0.921792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row3\" class=\"row_heading level0 row3\" >OOF Stacking</th>\n",
       "      <td id=\"T_95f6c_row3_col0\" class=\"data row3 col0\" >0.920343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row4\" class=\"row_heading level0 row4\" >Bayesian Weighted</th>\n",
       "      <td id=\"T_95f6c_row4_col0\" class=\"data row4 col0\" >0.919533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row5\" class=\"row_heading level0 row5\" >XGBoost</th>\n",
       "      <td id=\"T_95f6c_row5_col0\" class=\"data row5 col0\" >0.918236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row6\" class=\"row_heading level0 row6\" >RandomForest</th>\n",
       "      <td id=\"T_95f6c_row6_col0\" class=\"data row6 col0\" >0.898304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row7\" class=\"row_heading level0 row7\" >Pseudo-Labeling</th>\n",
       "      <td id=\"T_95f6c_row7_col0\" class=\"data row7 col0\" >0.553179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_95f6c_level0_row8\" class=\"row_heading level0 row8\" >Distilled Student</th>\n",
       "      <td id=\"T_95f6c_row8_col0\" class=\"data row8 col0\" >0.543091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2cb771d8ec0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaderboard = pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score']).sort_values(by='F1 Score', ascending=False)\n",
    "print(\"\\nüèÜ Ensemble Leaderboard:\")\n",
    "display(leaderboard.style.background_gradient(cmap='viridis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c94d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
